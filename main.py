# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXscR9tRjTabtA4ioGlPgENp0yy5gHJK
"""

from googleapiclient.discovery import build
import pandas as pd

# Set up YouTube API
api_key = 'YOUR_API_KEY'
youtube = build('youtube', 'v3', developerKey=api_key)

# Function to fetch video details
def fetch_video_details(video_ids):
    request = youtube.videos().list(
        part="snippet,statistics",
        id=','.join(video_ids)
    )
    response = request.execute()
    video_data = []
    for item in response['items']:
        video_info = {
            'video_id': item['id'],
            'title': item['snippet']['title'],
            'views': int(item['statistics'].get('viewCount', 0)),
            'likes': int(item['statistics'].get('likeCount', 0)),
            'comments': int(item['statistics'].get('commentCount', 0))
        }
        video_data.append(video_info)
    return pd.DataFrame(video_data)

# Data cleaning function
def clean_data(df):
    df.fillna(0, inplace=True)
    df[['views', 'likes', 'comments']] = df[['views', 'likes', 'comments']].astype(int)
    return df

# Example usage
video_ids = ['VIDEO_ID_1', 'VIDEO_ID_2']
video_data = fetch_video_details(video_ids)
clean_video_data = clean_data(video_data)

# Ranking function
def rank_videos(df):

    df['score'] = df['views'] * 0.5 + df['likes'] * 0.3 + df['comments'] * 0.2
    ranked_videos = df.sort_values(by='score', ascending=False).reset_index(drop=True)
    return ranked_videos

ranked_video_data = rank_videos(clean_video_data)

import openai
#Summarization using GPT-4

openai.api_key = "YOUR_OPENAI_API_KEY"

def summarize_video(title, description):
    prompt = f"Summarize the following YouTube video content:\nTitle: {title}\nDescription: {description}"
    response = openai.Completion.create(
        engine="gpt-4",
        prompt=prompt,
        max_tokens=150
    )
    summary = response.choices[0].text.strip()
    return summary

# Apply summarization for each video
ranked_video_data['summary'] = ranked_video_data.apply(lambda row: summarize_video(row['title'], row['title']), axis=1)

import streamlit as st
#User Interface with Streamlit
st.title("Top 10 YouTube Videos by Popularity Metrics")

def display_top_videos(df):
    st.write("### Top 10 Videos")
    for index, row in df.head(10).iterrows():
        st.subheader(f"{index + 1}. {row['title']}")
        st.write(f"Views: {row['views']}, Likes: {row['likes']}, Comments: {row['comments']}")
        st.write(f"Summary: {row['summary']}")
        st.write("---")

display_top_videos(ranked_video_data)

import time
#Evaluation
def evaluate_ranking_accuracy(ranked_data, expected_ranking):
    correct_ranking = sum(ranked_data.index == expected_ranking.index)
    accuracy = correct_ranking / len(ranked_data) * 100
    return accuracy

# Measure response time
start_time = time.time()
ranked_video_data = rank_videos(clean_video_data)
response_time = time.time() - start_time
print(f"Response time: {response_time:.2f} seconds")